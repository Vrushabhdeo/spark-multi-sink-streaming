{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99d514f1-cfa5-443f-94d3-8fe8c82fce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theory: \n",
    "\n",
    "# Spark Session Window in Structured Streaming\n",
    "# https://www.databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef725fa-3088-496f-99db-e71800fda2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://beb38ed1e6b8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Window Operation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffff82aac80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark Window Operation\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "    \n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528a927-c38c-4864-92ec-c64753243be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Topic payload \n",
    "# {\"event_time\": \"2024-04-09 12:00:00.000000\", \"data\": \"owl dog owl\"}\n",
    "# {\"event_time\": \"2024-04-09 12:03:00.000000\", \"data\": \"owl\"}\n",
    "# {\"event_time\": \"2024-04-09 12:05:00.000000\", \"data\": \"owl\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37541e6-b6ad-44c7-8e6e-d3cd9fadf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"wildlife-1\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "847d597c-ba14-4da2-9e78-6e8075c06d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary to string value column\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa2a5fe-1014-41ff-b121-3ea6b095d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, split, explode\n",
    "\n",
    "# JSON Schema\n",
    "json_schema = \"event_time string, data string\"\n",
    "\n",
    "# Expand JSON from Value column using Schema\n",
    "json_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a61582bc-acbe-42c4-bdf3-63b9a58579b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the required columns\n",
    "\n",
    "flattened_df = json_df.select(\"values_json.event_time\",\"values_json.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf3ea17-ffe9-43a9-9b80-a99581c3a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in words\n",
    "\n",
    "words_df = flattened_df \\\n",
    "    .withColumn(\"words\", split(\"data\", \" \")) \\\n",
    "    .withColumn(\"word\", explode(\"words\")) \\\n",
    "    .withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e7e0177-bcae-46f3-b578-520ee97f30ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a537e308-fb4d-49e3-8e0c-780459dc5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the words to generate count\n",
    "from pyspark.sql.functions import count, lit, window\n",
    "\n",
    "df_agg = words_df \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"10 minutes\", \"5 minutes\"),\n",
    "                          \"word\").agg(count(lit(1)).alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b91c55-e2b5-4765-967d-cb642bf844ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- word: string (nullable = false)\n",
      " |-- cnt: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df_agg.selectExpr(\"window.start as start_time\", \"window.end as end_time\", \"word\", \"cnt\")\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ab034b-2988-4736-93d0-8aa718ba4d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ffff8c4dba0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limitations - \n",
    "# Watermark discards old data, which Complete mode needs for correctness.\n",
    "# and hence we cannot use withWatermark with outputMode as \"complete\"\n",
    "# Avoid 'complete' mode in production environment as this may lead to OOM issue \n",
    "\n",
    "(df_final\n",
    " .writeStream\n",
    " .format(\"console\")\n",
    " .outputMode(\"complete\")\n",
    " .trigger(processingTime='30 seconds')\n",
    " .option(\"checkpointLocation\", \"checkpoint_dir_kafka_2\")\n",
    " .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a24f97-1f84-452f-b707-dd389ab5b67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ffff82a91b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_final\n",
    " .writeStream\n",
    " .format(\"console\")\n",
    " .outputMode(\"update\")\n",
    " .trigger(processingTime='30 seconds')\n",
    " .option(\"checkpointLocation\", \"checkpoint_dir_kafka_3\")\n",
    " .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c05d2f-e71c-43b7-9d15-b63eb70746b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "25/03/09 12:08:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
    "2025-03-09 17:38:10 -------------------------------------------\n",
    "2025-03-09 17:38:10 Batch: 0\n",
    "2025-03-09 17:38:10 -------------------------------------------\n",
    "2025-03-09 17:38:11 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:11 |         start_time|           end_time|word|cnt|\n",
    "2025-03-09 17:38:11 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:11 |2024-04-09 11:55:00|2024-04-09 12:05:00| dog|  1|\n",
    "2025-03-09 17:38:11 |2024-04-09 11:55:00|2024-04-09 12:05:00| owl|  2|\n",
    "2025-03-09 17:38:11 |2024-04-09 12:00:00|2024-04-09 12:10:00| owl|  2|\n",
    "2025-03-09 17:38:11 |2024-04-09 12:00:00|2024-04-09 12:10:00| dog|  1|\n",
    "2025-03-09 17:38:11 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:11 \n",
    "2025-03-09 17:38:15 \n",
    "[I 2025-03-09 12:08:15.954 ServerApp] Saving file at /spark-streaming/08_window_operations_and_watermarks.ipynb\n",
    "2025-03-09 17:38:18 -------------------------------------------\n",
    "2025-03-09 17:38:18 Batch: 0\n",
    "2025-03-09 17:38:18 -------------------------------------------\n",
    "2025-03-09 17:38:19 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:19 |         start_time|           end_time|word|cnt|\n",
    "2025-03-09 17:38:19 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:19 |2024-04-09 11:55:00|2024-04-09 12:05:00| dog|  1|\n",
    "2025-03-09 17:38:19 |2024-04-09 11:55:00|2024-04-09 12:05:00| owl|  2|\n",
    "2025-03-09 17:38:19 |2024-04-09 12:00:00|2024-04-09 12:10:00| owl|  2|\n",
    "2025-03-09 17:38:19 |2024-04-09 12:00:00|2024-04-09 12:10:00| dog|  1|\n",
    "2025-03-09 17:38:19 +-------------------+-------------------+----+---+\n",
    "2025-03-09 17:38:19 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
