{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a70373-aafa-410b-88b9-a5cd5397f19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://beb38ed1e6b8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Kafka Multiple Sink</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffff82d9090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Kafka Multiple Sink\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\n",
    "    .config('spark.jars', \"/opt/spark/jars/postgresql-42.2.20.jar\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c4c658-2f5a-4665-bcd0-d9e6456df92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "raw_kafka_df = (spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "            .option(\"subscribe\", \"device-data\")\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de8a421-f825-45b4-90da-273e97d6953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined logic for handling the error records\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import from_json, col, expr, explode, current_timestamp, lit, size\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "def flatten_data(df):\n",
    "    raw_kafka_df = raw_kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "    json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId', StringType(), True), \n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices', \n",
    "                     ArrayType(StructType([ \n",
    "                        StructField('deviceId', StringType(), True), \n",
    "                        StructField('measure', StringType(), True), \n",
    "                        StructField('status', StringType(), True), \n",
    "                        StructField('temperature', LongType(), True)\n",
    "                    ]), True), True)\n",
    "        ]), True), \n",
    "    StructField('eventId', StringType(), True), \n",
    "    StructField('eventOffset', LongType(), True), \n",
    "    StructField('eventPublisher', StringType(), True), \n",
    "    StructField('eventTime', StringType(), True)\n",
    "    ])\n",
    "    )\n",
    "    raw_kafka_df = raw_kafka_df.withColumn(\"value_json\", from_json(col(\"value\"), json_schema))\n",
    "    \n",
    "    # Filter out for error data\n",
    "    error_df = raw_kafka_df.select(\"key\", \"value\").withColumn(\"eventtimestamp\",lit(current_timestamp())) \\\n",
    "        .where(\"values_json.customerId is null or size(values_json.data.devices) = 0\")\n",
    "\n",
    "    # Filter out correct flattened data\n",
    "    streaming_df = raw_kafka_df.where(\"values_json.customerId is not null and size(values_json.data.devices) > 0\") \\\n",
    "        .selectExpr(\"values_json.*\")\n",
    "    \n",
    "    stream_exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    stream_flattened_df = (\n",
    "    stream_exploded_df\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop('data', 'data_devices')\n",
    "    )\n",
    "    return stream_exploded_df, error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45ce160-5e7a-4575-975a-261ecd7d96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write the dataframe to JDBC (Postgres)\n",
    "\n",
    "def write_on_postgres(df, table_name):\n",
    "    (\n",
    "        df\n",
    "        .write\n",
    "        .format(\"jdbc\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost:5432/sqlpad\")\n",
    "        .option(\"dbtable\", table_name)\n",
    "        .option(\"user\", \"sqlpad\")\n",
    "        .option(\"password\", \"sqlpad\")\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c2e681-8ec7-4c98-9dd7-5ef11e342cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Error and Exception and write to JDBC \n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def write_multiple_sink(df, batch_id):\n",
    "    print(\"Processing batch Id: \" + str(batch_id))\n",
    "    try:\n",
    "        # Get the Flattened and Error Dataframe\n",
    "        flattened_df, error_df_raw = flatten_data(df)\n",
    "        \n",
    "        # Add the batchid column in Error Dataframe\n",
    "        error_df_raw = error_df_raw.withColumn(\"batch_id\", lit(batch_id))\n",
    "\n",
    "        # Write Flattened Dataframe to JDBC\n",
    "        write_on_postgres(flattened_df, 'device_data')\n",
    "\n",
    "        # Write Error Datafram to JDBC\n",
    "        write_on_postgres(error_df_raw, 'device_data')\n",
    "    \n",
    "        # Display both Dataframes for confirmation\n",
    "        flattened_df.show(truncate=False)\n",
    "        error_df_raw.show(truncate=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        (df\n",
    "         .write\n",
    "         .format(\"parquet\")\n",
    "         .mode(\"append\")\n",
    "         .save(\"data/07_handling_errors_and_exceptions/output/unprocessed_device_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adc08a-3751-4224-8e72-962ad6514bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running foreachBatch\n",
    "# Write the output to Multiple Sinks\n",
    "\n",
    "( stream_flattened_df\n",
    " .writeStream\n",
    " .foreachBatch(write_multiple_sink)\n",
    " .option(\"checkPointLocation\", \"checkpoint_dir\")\n",
    " .trigger(processingTime='10 seconds')\n",
    " .start()\n",
    " .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9f695-261e-4942-99ce-3f1a7d481952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
