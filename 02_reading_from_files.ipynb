{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e82ed6-9268-46c1-b358-fa3e58fee27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the streaming_df to read from input\n",
    "# print the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2192a8ef-2de7-434e-ade7-24563b98cd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://beb38ed1e6b8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffff82d8370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Streaming Process Files\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b52005-3adb-4881-8cfa-407926cec03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "|customerId|                data|             eventId|eventOffset|eventPublisher|           eventTime|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "|   CI00103|{[{D001, C, ERROR...|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+--------------------+\n",
      "|customerId|                data|             eventId|eventOffset|eventPublisher|           eventTime|        data_devices|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+--------------------+\n",
      "|   CI00103|{[{D001, C, ERROR...|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|{D001, C, ERROR, 15}|\n",
      "|   CI00103|{[{D001, C, ERROR...|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|{D002, C, SUCCESS...|\n",
      "+----------+--------------------+--------------------+-----------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Batch code\n",
    "# Explode the 'devices' field containing list/array\n",
    "# print the schema\n",
    "\n",
    "batch_df = spark.read.format(\"json\").load(\"data/02_reading_from_files/input/\")\n",
    "batch_df.printSchema()\n",
    "batch_df.show(truncate=True)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "batch_exploded_df = batch_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "batch_exploded_df.printSchema()\n",
    "batch_exploded_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fa78d9-a4fe-44e0-a862-4c1db52ce0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "|customerId|             eventId|eventOffset|eventPublisher|           eventTime|deviceId|measure| status|temperature|\n",
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "|   CI00103|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|    D001|      C|  ERROR|         15|\n",
      "|   CI00103|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|    D002|      C|SUCCESS|         16|\n",
      "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten the exploded 'data_devices' column\n",
    "# print the schema\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "batch_flattened_df = (\n",
    "    batch_exploded_df\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop('data', 'data_devices')\n",
    ")\n",
    "\n",
    "batch_flattened_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfeebd-e06f-43b1-a1f6-ad5bd3e40cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887793d4-72c9-4699-b2ae-a77eefd9ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Stream code\n",
    "\n",
    "# set the inferSchema as true. put option cleanSource, sourceArchiveDir and maxFilesPerTrigger\n",
    "# cleanSource: noop/archive/delete\n",
    "# sourceArchiveDir: path to store the archieved files\n",
    "# maxFilesPerTrigger: no of files taken as a input at some point of time \n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "streaming_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .option(\"cleanSource\", \"archive\")\n",
    "    .option(\"sourceArchiveDir\", \"arch_dir\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .format(\"json\")\n",
    "    .load(\"data/02_reading_from_files/input/\")\n",
    ")\n",
    "\n",
    "streaming_df.printSchema()\n",
    "\n",
    "# Explode the 'devices' field containing list/array\n",
    "# print the schema\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "stream_exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "stream_exploded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc0e0de-b6a0-4946-b692-b5eee55aa850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten the exploded 'data_devices' column\n",
    "# print the schema\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "stream_flattened_df = (\n",
    "    stream_exploded_df\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop('data', 'data_devices')\n",
    ")\n",
    "\n",
    "stream_flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2866d47-9eb1-438a-961d-09a399f2e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to console sink \n",
    "\n",
    "( stream_flattened_df\n",
    " .writeStream\n",
    " .format(\"console\")\n",
    " .outputMode(\"append\")\n",
    " .option(\"checkPointLocation\", \"checkpoint_dir\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c13ec-8584-4a2d-bb4c-6f2a8556515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# console output: \n",
    "\n",
    "[I 2025-03-06 13:20:42.795 ServerApp] Saving file at /spark-streaming/02_reading_from_files.ipynb\n",
    "25/03/06 13:21:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
    "-------------------------------------------\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|customerId|             eventId|eventOffset|eventPublisher|           eventTime|deviceId|measure| status|temperature|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|   CI00103|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|    D001|      C|  ERROR|         15|\n",
    "|   CI00103|e3cb26d3-41b2-49a...|      10001|        device|2023-01-05 11:13:...|    D002|      C|SUCCESS|         16|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "[I 2025-03-06 13:21:49.594 ServerApp] Saving file at /spark-streaming/02_reading_from_files.ipynb\n",
    "[I 2025-03-06 13:22:17.122 ServerApp] Saving file at /spark-streaming/02_reading_from_files.ipynb\n",
    "[I 2025-03-06 13:22:29.842 ServerApp] Copying 'spark-streaming/data/02_reading_from_files/samples/device_02.json' to '/spark-streaming/data/02_reading_from_files/input'\n",
    "25/03/06 13:23:26 WARN FileStreamSource$SourceFileArchiver: Fail to move file:/home/jupyter/spark-streaming/data/02_reading_from_files/input/device_01.json to file:/home/jupyter/spark-streaming/arch_dir/home/jupyter/spark-streaming/data/02_reading_from_files/input/device_01.json / skip moving file.\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|customerId|             eventId|eventOffset|eventPublisher|           eventTime|deviceId|measure| status|temperature|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|   CI00108|aa90011f-3967-496...|      10003|        device|2023-01-05 11:13:...|    D004|      C|SUCCESS|         16|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|customerId|             eventId|eventOffset|eventPublisher|           eventTime|deviceId|measure| status|temperature|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "|   CI00108|aa90011f-3967-496...|      10003|        device|2023-01-05 11:13:...|    D004|      C|SUCCESS|         16|\n",
    "+----------+--------------------+-----------+--------------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "[I 2025-03-06 13:24:14.107 ServerApp] Saving file at /spark-streaming/02_reading_from_files.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c06c2-9075-423c-b226-8db18c17ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output in csv format\n",
    "(stream_flattened_df\n",
    " .writeStream\n",
    " .format(\"csv\")\n",
    " .outputMode(\"append\")\n",
    " .option(\"path\", \"data/02_reading_from_files/output/device_data.csv\")\n",
    " .option(\"checkPointLocation\", \"checkpoint_dir\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a6a5a-aa11-4229-b728-0453164e468c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
